---
title: 概率密度估计的非参数方法
date: 2019-07-01 12:00:00 +0800
categories: blogs
tags: [pattern-recognition, statistical-inference ]
---

# 概率密度估计的非参数方法

## 1. 问题定义：

给定观测样本$x_1, x_2, ..., x_N$，设其独立同分布(iid)，但分布形式未知，如何从样本中估计出概率密度函数(pdf) ?
    
不用对pdf的形式做任何假设，直接用样本估计出整个函数。某种意义上，非参数估计也可以理解为无限参数的估计，因为对pdf形式的假设其实给分布定下了很强的限制，参数个数一般决定了模型复杂度，而非参数估计的限制非常弱，可以认为模型复杂度无穷大。

<!-- more -->

## 2. 基本思路

考虑一个小区域R，某个随机向量落入这个区域的概率是
$$P_R=\int_R{p(\boldsymbol{x})d\boldsymbol{x}}$$
（这里$x$可以是标量或向量，我懒得打\boldsymbol了，后面就当做标量吧，不影响结论）

假定$p(x)$连续，则当R足够小，可以认为在该范围内$p(x)$没有变化，是常数。则上式可近似为
$$P_R=\int_R{p(\boldsymbol{x})d\boldsymbol{x}}=p(x)V$$
其中$V=\int_R{dx}$，是所选区域的体积。那么也就有$p(x)=\frac{P_R}{V}$

这里问题就是，$P_R$和$V$分别怎么估计

## 3. 方法举例
###3.1. 直方图法
**方法**：
（1）把样本$x$在其取值范围内分割成$M$个等间隔的小窗，如果$x$是$d$维向量则把每一维都分成$M$等份，这样就得到$M^d$个小窗，因为都是超立方体，体积$V$也很好求，且每个小窗的体积是一样的
（2）统计落入每个小窗的样本数$k_m$
（3）
$$\hat p(x)=\frac{k}{NV}$$

其中$\hat p(x)$是对$x$位置的pdf的估计，$k$为落入$x$所在小窗的样本点的个数，$N$为样本总个数，$V$为小窗的体积。

**解释**：

$P_R$的估计：
样本集中，落入这个区域R的点的个数$k$服从二项分布，即 $k \thicksim B(N,P_R)$，那么$\frac{k}{N}$就是二项分布下$P_R$的无偏估计和极大似然估计了

$V$的估计：
没有什么好估计的，每个小窗的$V$都一样，是划分小窗的时候就固定的

**优缺点**：
优点：
（1）最简单直观，也是日常用得最多的
（2）理论上，当样本量趋于无穷多，$\hat p(x)$可以收敛于$p(x)$
缺点：
（1）小窗数的选择影响很大，选择过小会导致估计的pdf非常粗糙，选择过大会导致有一些小窗内没有样本或者样本很少，使得估计的pdf很不连续
（2）收敛的条件是：a). ${lim \atop {n->\infty}} V_n=0$, b). ${lim \atop {n->\infty}} k_n = \infty$, c). ${lim \atop {n->\infty}} \frac{k_n}{n}=0$。即要求样本无穷多，小窗体积无穷小，每个小窗里的点无穷多，但相对总样本的比例无穷小。这个要求导致小窗的选取要与样本数、样本分布都有关，非常不方便。
（3）由于概率分布有高密度区域和低密度区域，也很有可能小窗数会相对高密度区域过小，相对低密度区域又过大

![直方图法](/assets/images/2019-07-01-密度估计的非参数方法.md/1.png)

### 3.2. $k_N$近邻估计法
**方法**：
（1）对于任意坐标$x$，计算其附近的$k_N$个样本点，这个$k_N$是根据$N$的数量由用户自己设定的参数，常见的选择策略是$k_N\thicksim k\sqrt{N}$
（2）包含这k个样本点的最小小窗的体积记为$V$

**解释**：
和直方图法正好相反，$k_N$法固定了$P_R=\frac{k_N}{N}$，主要估计的是$V$，也比较直观。此外，$V$会在密度高的地方较小而在密度低的地方自动增大。

**优缺点**：
能比较好地兼顾高密度与低密度区域估计的连续性

### 3.3. parzen窗法
**方法**：
$$\hat p(x)=\frac{1}{N}\Sigma_{i=1}^{N}{K(x,x_i)}$$

其中$K(x,x_i)$叫窗函数，是需要满足pdf要求的函数，即
$$K(x,x_i)\geq0 \text{ ，且} \int{K(x,x_i)dx=1}$$
其几种常见的窗函数：
(1)方窗

$$
\begin{equation} 
k(x_,x_i) = \begin{cases}
   \frac{1}{h^d} &\text{if } |x^j-x^j_i\leq h/2|, j=1,2,...,d \\
   \\
   0 &\text{else} 
\end{cases} 
\end{equation} 
$$

(2)高斯窗
$$k(x,x_i)=\frac{1}{\sqrt{(2\pi)^d\rho^{2d}|Q|}}\exp{\lbrace -\frac{(x-x_i)^TQ^{-1}(x-x_i)}{2\rho^2}\rbrace}$$

(3)超球窗
$$
\begin{equation} 
k(x_,x_i) = \begin{cases}
   V^{-1} &\text{if } ||x-x_i\leq \rho|| \\
   \\
   0 &\text{else} 
\end{cases} 
\end{equation} 
$$
其中$V$是超球的体积，$\rho$是超球半径

**解释**：
一般来说窗函数是$||x-x_i||$的函数，我认为可以从两个角度理解，对于待估计的任意一点$x$，窗函数是把整个空间赋予了权重，样本点以其所在位置的权重贡献给$P_R$，而权重的要求是其全定义域积分为1，所以$V=1$。

从样本的角度，每个样本点以一个“窗函数”的形式对定义域里的所有位置产生影响，当然像方窗这样设定阈值，阈值之外贡献都是0的例子也有。下图示意的是高斯窗的场景。

![此处输入图片的描述](/assets/images/2019-07-01-密度估计的非参数方法.md/2.png)

**优缺点**：
每一个位置的pdf估计都用上了所有样本的信息，是这几种方法里最有效利用了信息的一个。窗函数及其参数选的好，可以保证pdf的连续性，甚至用很小的样本量就学到非常逼近的pdf。

但是窗函数的选取其实应该包含着使用者对pdf的某种假设，这与非参数学习的思想不是特别契合。
